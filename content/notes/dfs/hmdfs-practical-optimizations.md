---
title: "轻量级分布式文件系统在消费级设备上的实用优化技术"
date: 2025-10-15
draft: false
tags: ["HMDFS", "分布式文件系统", "NAS", "缓存优化", "文件系统调度"]
categories: ["分布式系统", "文件存储"]
math: true
---
# 轻量级分布式文件系统在消费级设备上的实用优化技术
## 一、研究背景
### 1.1 核心问题：家庭NAS的体验瓶颈
**场景定义：**

随着消费级设备（手机、平板、智能手表等）的普及，用户每天都会生成大量数据。然而，本地硬件容量有限，很难长期满足存储需求。为此，家庭用户通常会搭建 NAS（Network Attached Storage），即一个中心存储节点（NAS 服务器）与多个消费设备（客户端）组成的分布式系统，客户端通过无线局域网访问共享文件。

**三大挑战：**
- 网络不稳定：设备频繁上下线，带宽波动大
- 内存有限：因为内存小，很容易被占满，系统为了腾出空间而频繁清理缓存，使得本应被加速的操作变回缓慢的原始操作，导致用户体验下降
- 带宽有限：多人同时读写，排队等待时间长

为协调 NAS 与客户端，系统需要分布式文件系统，常见方案如 AFS、NFS 等，但它们主要面向高带宽数据中心，不适用于移动端与家庭设备。
### 1.2 系统基础：HMDFS 架构解析
Harmony Distributed File System（HMDFS）是华为在 HarmonyOS（鸿蒙操作系统）中实现的一种轻量级分布式文件系统，旨在为消费级设备提供高效、低开销的数据备份与共享解决方案。 

#### 设计理念
**（1）内核堆叠式架构（Stacked In-Kernel Design）**
HMDFS 并非独立的用户态应用，而是直接集成在操作系统内核中。它以堆叠层的形式运行在其他文件系统之上，从而：
- 减少远程文件访问的系统调用与上下文切换开销
- 让分布式访问对上层应用完全透明
- 保持低延迟与轻量化运行特性

**（2）轻量化与无感接入**
所有分布式文件访问逻辑均在内核态完成，无需安装额外守护进程或用户态服务。  
这意味着：
- 更低的内存占用
- 更少的后台线程
- 更符合移动端与 IoT 场景的资源约束

#### 客户端工作流程
1. 应用发出读/写系统调用  
2. VFS 调用 HMDFS 实现的对应操作 
3. HMDFS 通过网络将请求发送至服务器
4. 服务器响应后，文件数据被缓存至页面缓存
5. 应用从缓存中读取数据完成操作

#### 服务器端工作流程
1. 服务器接收客户端请求 
2. HMDFS 解析请求内容
3. 通过 VFS 接口访问底层备份文件系统
4. 执行具体文件操作并返回响应

#### 优势与局限
HMDFS 专为移动场景设计，采用简化的客户端–服务器通信协议。  

这种架构具备以下优点：
- 实现轻量化运行
- 避免数据中心复杂的并行与一致性机制
- 更适合资源受限的终端设备

但同时也暴露出若干待优化问题：
- 缓存不足：VFS 提供的页面缓存容量有限，频繁淘汰导致远程访问开销高。 
- 网络不稳定：移动网络波动大，易造成访问延迟与断连。 
- 写回机制简单：当前写回策略未考虑任务特性（如优先级、依赖），整体效率偏低。 
- 缺乏并发控制：多设备同时修改共享文件时可能导致冲突与数据损坏。 

## 二、四类优化
### 2.1 基于交换区的持久化缓存（Swap-based Persistent Cache）
#### 问题背景：为什么要用 Swap 做缓存？
传统操作系统中，swap 区用于把匿名页（程序运行时动态生成、没有文件来源的内存页，只能通过 swap 区保存）换出到磁盘，释放内存。而文件页（由某个磁盘文件提供内容支撑的内存页）可以从文件系统重新读取回来，因此通常不需要放到swap。

但在 HMDFS 中，文件页来自远程服务器。当这些远程文件页被从内存中淘汰时，下一次访问就得重新发网络请求取回它们。

因此，如果能像内存 swap 一样，把这些远程页在本地换出并持久化保存，下次访问就能直接从本地恢复，不用再走网络。
#### 算法1：优化的读文件页流程
检查目标页是否存在于本地 swap 区：
- 如果命中：
    - 从 swap 区读取该页 read_from_swap(file, page)
    - 更新 LRU 列表中的命中状态 lru_list_hit(...)
- 否则：
    - 从远程服务器读取 read_from_remote(file, page)
    - 异步写入本地 swap write_to_swap(...)
#### 算法2：write_to_swap
这个函数负责将远程页写入本地持久化缓存。
1. 写入 swap(file, page)
2. 更新当前 swap 占用量 Ccur += PAGE_SIZE
3. 若超过最大容量 Cmax，淘汰 LRU 尾部页面
4. 将当前文件页加入 LRU 表头
### 2.2 基于 Markov 方法的跨设备文件预取（Cross-device Prefetch with Markov Method）
#### 问题背景：为什么需要跨设备预取？
**分布式访问的瓶颈**

在 HMDFS 这样的分布式文件系统中，文件分布在不同设备节点上（可能是手机、平板、NAS、路由器等），当客户端访问一个远程文件时，需要跨网络发送请求。因此，不可避免会出现网络延迟。

**解决思路：提前预取**

如果系统能预测下一个要访问的文件，并提前把它的内容在本地缓存好。这样，在用户请求真正到来前，数据已在本地，实际访问时几乎无等待。
#### 传统方法的局限：Linux Readahead 机制不够智能
Linux 的 readahead 是一种顺序预读机制，它预先读取同一文件的连续页。

优点是对流式读很有效（如播放视频、顺序读大文件）。

缺点是不能预测跨文件访问，对随机访问负载无效。它无法应对“先打开 A 文件，再马上打开 B 文件”的使用习惯。

因此，HMDFS 希望通过学习用户访问序列规律来预测未来访问的文件。

#### 核心思想：用 Markov 模型预测下一个文件
**Markov 方法简介**

Markov 模型（马尔可夫链）是一种基于历史序列的概率预测方法：未来的状态只依赖于当前状态，而与更早的历史无关。

**在 HMDFS 中的做法**

1. 收集访问历史
每当用户打开文件，就记录“上一个文件 → 当前文件”的转移。例如，File1 → File2 的转移计数 +1。

2. 维护 Markov 转移表
每个文件对应一个唯一索引，转移表记录从每个文件到其他文件的转移概率，存放在客户端本地，形成文件访问状态模型。

3. 预测下一个文件
根据当前访问文件，从转移表中找到下一个最可能访问的文件。预取该文件的部分内容到本地缓存。默认预取文件的前两页（可动态调整）。

#### 预取过程的完整工作流
整个机制分为三部分：

**（1）构建预测模型**
- 实时监控 HMDFS 的远程文件读请求
- 利用哈希表快速索引文件
- 每个文件分配一个唯一索引，作为 Markov 表的下标
- 动态更新转移计数

**（2）执行文件预取**
- 根据当前访问的文件，计算可能的下一个文件
- 根据网络带宽、延迟动态决定预取数量
- 默认预读前两页以确保文件能快速打开
- 之后由 Linux readahead 继续向后预取同一文件的后续页

**（3）模型自适应与冷启动**
- 在初期访问历史不足时，预测精度较低
- 随着访问记录累积，模型逐步学习用户习惯
- 如果一段时间内命中率持续下降，说明用户行为变了，自动重建模型
### 2.3 任务感知的回写调度（Task-aware Write-back Scheduling）
#### 问题背景：缓存回写（write-back）的瓶颈
在 HMDFS 中，客户端的文件写入通常采用缓冲写（buffered I/O）。写操作先写入内存中的缓存页，系统稍后再通过后台线程将这些页回写（write-back）到服务器，这能加快用户操作的即时响应。

但问题在于客户端可能同时有多个进程在写不同文件。系统用一个延迟任务队列顺序处理这些回写任务，该队列默认是 FCFS（先来先服务）。若一个大文件写回耗时长，后面的任务就被一直堵住。于是出现了性能问题：缓存同步延迟高、任务等待时间长，导致文件状态更新慢。

#### 优化思路：像进程调度一样调度回写任务
HMDFS 借鉴了操作系统的进程调度算法：把每个“文件回写任务”当成一个“待运行的进程”，然后按任务特性来分配调度优先级。

SJF（Shortest Job First）理论上能带来最小平均等待时间。但它也有缺陷，长任务可能一直得不到执行。所以 HMDFS 设计了一个改进版：同时考虑任务大小和老化时间（aging）的任务感知调度策略。
#### 优先级模型
**（1）优先级公式**
$$
\text{Priority} =
\begin{cases}
N_{\text{dirty}}, & t \le \delta \\
-\,t, & t > \delta
\end{cases}
$$
其中：
- \(N_{\text{dirty}}\)：任务中需要回写的页数量。页越多，任务越大，优先级越低。
- t：任务的老化度（aging degree），表示该任务在队列中等待的时间。等待时间越久，优先级上升。
- δ：可配置的老化阈值，控制何时让长任务上升优先级，避免长期被阻塞。

原则：数值越小优先级越高。

**（2）老化度计算公式**
$$
t =
\begin{cases}
0, & T_{\text{LastMod}} \le T_{\text{LastSync}} \\
T_{\text{LastMod}} - T_{\text{LastSync}}, & T_{\text{LastMod}} > T_{\text{LastSync}}
\end{cases}
$$
含义：
- \(T_{\text{LastMod}}\)：文件最后修改时间
- \(T_{\text{LastSync}}\)：文件上次同步时间

如果文件自上次同步后又被修改，则 t = \(T_{\text{LastMod}} - T_{\text{LastSync}}\)

否则 t = 0
#### 执行逻辑
1. HMDFS 为每个写回任务建立队列项
2. 系统定期扫描队列，根据公式计算每个任务的 Priority
3. 调度器优先处理 Priority 较小（优先级高）的任务
4. 当任务等待时间过长（t > δ）时，即使它很大，也会被提前执行
5. 随着任务完成，更新 T_LastSync 和队列状态
### 2.4 基于 MRSW（Multiple Readers, Single Writer）协议的多设备并发访问控制（ MRSW‑based Protocol for Multi‑device Access）
#### 问题背景：多设备访问导致的冲突与不一致
在 HMDFS 中，文件可以被多个客户端设备（比如手机、平板、电视、NAS 等）同时访问。

但这带来了两个严重问题：
1. 并发写入冲突（write conflict）

若多个设备同时写入同一文件，文件内容会被破坏，因此必须限制写者数量为 1。

2. 文件视图不一致（inconsistent view）

当一个设备修改文件后，其他设备看到的仍是旧版本。需要一种机制来保证所有客户端最终一致。

于是 HMDFS 引入了一个轻量化分布式锁机制：MRSW 协议（Multiple Readers, Single Writer Protocol）

#### 基本思想：MRSW（多读单写）
允许多个客户端同时以只读（Read-Only, RO）方式访问同一文件，同一时间仅允许一个客户端拥有写权限（Write-Only, WO）。任意时刻，某个文件最多只有一个写者，但可以有多个读者。

#### HMDFS 的 MRSW 扩展机制
HMDFS 在通信协议中嵌入了两套子协议：
- 客户端子协议控制客户端的文件访问状态（只读 / 读写）
- 服务器子协议统一管理文件写权限分配、过期与回收

这两部分协作构成了一个分布式状态机（state machine）。

#### MRSW 协议的服务器端逻辑与机制概要
服务器作为中心协调者，负责统一管理文件的访问权限与写权限分配。其逻辑可概括为三个核心环节：授权 → 续期 / 回收 → 状态转换。

首先，系统初始化权限状态表 Ps 与过期时间表 T，并设置写权限的有效期 texp。

当客户端请求写入文件时：
- 若文件当前无写者，服务器授予该客户端读写权限 (RO | WO)，并设置过期时间
- 若当前客户端已持有写权限且仍在有效期内，则自动续期，避免频繁重授权
- 若其他客户端的写权限已过期，则撤销旧权限并重新分配给新客户端

服务器持续监控所有权限状态：一旦检测到超时，立即回收写权限。此时空闲文件可被其他客户端抢占写权限。

为防止死锁（如客户端掉线或长时间不写），HMDFS 为每个写权限设置过期机制：
- 若客户端持续写入，权限自动续期
- 若长时间无活动，则自动失效

这样既能保证并发访问安全，又能在离线或弱网环境中保持系统活性。

从状态机角度看：
- 服务器状态机：每个状态节点表示当前写权限归属哪个客户端，状态转换由“超时、请求、续期”等事件触发

- 客户端状态机：状态在 RO ↔ (RO | WO) 之间切换，转换条件为“申请写权限、获得授权或权限过期回收”

综上，MRSW 协议通过“单写多读 + 权限过期重分配”机制，在保证数据一致与并发安全的同时，有效避免死锁与长时占用问题。