---
title: "星际文件系统 (IPFS) 简介"
date: 2025-10-14
draft: false
tags: ["IPFS", "分布式文件系统", "去中心化存储", "内容寻址", "区块链"]
categories: ["分布式系统", "文件存储"]
---
# 星际文件系统（IPFS）简介
## 一、IPFS 是什么
星际文件系统（InterPlanetary File System，简称 IPFS），是一种去中心化、内容可寻址的对等超媒体分发协议。它试图构建一个全球统一的分布式文件系统，让所有计算设备都能像访问本地磁盘一样访问网络中的数据。

IPFS 是一种新型互联网基础设施协议。它让文件的位置寻址变成内容寻址：不再通过服务器地址或 URL 来获取资源，而是通过文件内容的哈希值来直接定位数据。

项目由 Juan Benet 于 2014 年在 Protocol Labs 启动，并在开源社区的支持下不断发展。当前主要实现版本为 Go-IPFS（Go 实现）与 JS-IPFS（JavaScript 实现），Python 版本也在社区维护。

## 二、技术机制
### 2.1 内容寻址
核心功能：通过内容的哈希标识唯一文件，而不是依赖其存储位置。

在传统 HTTP 中，资源通过位置（URL）访问。这种方式依赖服务器位置，一旦服务器宕机或路径变化，资源就无法访问。
而在 IPFS 中，资源由内容哈希（CID，Content Identifier）唯一标识。无论它存储在全球哪个节点，只要内容一致，CID 就完全相同。

工作流程：

（1）分块存储

文件在上传时会被分割成若干个256KB 的小块（block）。

（2）哈希计算

每个块计算一个唯一的哈希值（通常基于 SHA-256）。

（3）这些块的哈希组成一个树状结构（Merkle DAG）

块的哈希再组合为一个树状结构（Merkle DAG），用于追踪文件整体结构。

（4）形成 CID（Content Identifier）

文件的根节点哈希即为 CID。CID 与内容一一对应：相同内容 → 相同 CID。
### 2.2 Merkle DAG
核心功能：将文件及其块通过哈希链接形成可验证、可复用的图结构。
它是 IPFS 实现数据完整性、去重存储和版本控制的关键基础。

Merkle DAG是一棵哈希树：
```
          Root (文件CID)
             │
   ┌─────────┴─────────┐
   │         │         │
 Block1     Block2    Block3 ...
 (h1)       (h2)       (h3)
```
每个叶子节点存储实际数据块及其哈希，上层节点仅包含子节点哈希列表，根节点的哈希（Root CID）代表整个文件的唯一身份。

**文件哈希如何由块组成**

假设上传一个文本文件：
```
Hello, this is IPFS!
```

它被拆分为两个块：
```
Block1 = "Hello, this "
Block2 = "is IPFS!"
```

IPFS 会计算每个块的哈希：
```
h1 = hash("Hello, this ")
h2 = hash("is IPFS!")
```

然后创建一个根节点（文件对象），记录这些块：
```
Root Node:
{
  Data: null,
  Links: [h1, h2]
}
```

再对整个根节点结构求哈希 → 得到文件的 CID。
这就形成了一棵简单的 Merkle DAG：
```
    Root CID
    /    \
   h1     h2
```

如果文件内容哪怕改动一个字，例如改成 “Hello, this is IPFS?”（句号变问号），Block2 的哈希 h2 就完全不同，根哈希也随之变化。因此，IPFS 能立即检测到文件被修改。

**块是全网共享的内容单元**

在 IPFS 中，块是全网共享的内容单元。任何文件、目录或用户，只要含有相同的内容片段，都会引用同一个块哈希。

假设有两个文件：
```
A.txt = "ABCDEFGH"
B.txt = "ABCD1234"
```
IPFS 拆分后：
```
块1: "AB" → h1
块2: "CD" → h2
块3: "EF" → h3
块4: "GH" → h4
块5: "12" → h5
块6: "34" → h6
```

对应结构：
```
A.txt → [h1, h2, h3, h4]
B.txt → [h1, h2, h5, h6]
```

h1 ("AB") 和 h2 ("CD")被两个文件共同引用，它们只在网络中保存一份，被多个文件节点共享使用。
```
┌────────────┐
│  Block h1  │  "AB"
└────────────┘
    ▲        ▲
    │        │
A.txt      B.txt
```

这种机制称为内容去重（Deduplication），是 IPFS 存储高效和节省空间的关键。
### 2.3 分布式哈希表（DHT：Distributed Hash Table）
**基本概念**

在 IPFS 中，分布式哈希表（DHT）是整个网络中用于内容定位和节点寻址的核心组件。
内容寻址与 Merkle DAG 已经解决了“内容是什么”的问题，而 DHT 则负责回答“内容在哪儿”。

在传统的中心化网络中，文件索引通常保存在服务器或数据库中；而在 IPFS 中，这张索引表被拆分并分布到全球各个节点中。

每个节点都维护一小部分映射关系，结构如下：
```
<Key: CID> → <Value: PeerID>
```
其中：
- CID（Content Identifier） 表示文件的内容哈希
- PeerID 表示存储该内容的节点身份

当用户请求某个 CID 时，IPFS 通过 DHT 网络在节点之间进行查询，最终找到持有该内容的节点。

**工作原理：去中心化的全网索引**

在 DHT 中，每个节点都掌握网络索引表的一部分。
- 任何节点都可以根据内容哈希查询存储该内容的节点
- 查询请求会在节点之间逐跳传播
- 通过路由算法，查询可在较短时间内到达目标节点

这种方式避免了中心化目录服务器，提升了系统的可扩展性与容错能力。

**Kademlia 协议**

IPFS 的 DHT 基于 Kademlia 协议实现。该协议通过一种高效的异或距离算法来度量节点之间的接近程度。

（1）异或距离定义：

每个节点和内容哈希都可以看作一个大整数，节点之间的距离定义为：
```
Distance(A, B) = XOR(A, B)
```
距离越小，表示节点越接近目标内容。

（2）路由表结构：

每个节点维护一个有限大小的路由表，被划分为若干个桶。每个桶负责保存特定距离范围内的节点信息。
这样，节点既能快速找到邻近节点，也能通过分布式路由访问远程节点，从而形成一种局部紧密、全局可达的网络结构。

（3）查找过程：

当节点需要查找某个 CID 时：

1. 根据本地路由表，选出距离 CID 最近的若干节点

2. 向这些节点发送查找请求

3. 每个节点返回自己所知的更接近目标的节点

4. 过程递归进行，直到找到持有该 CID 内容的节点。

整个查找的时间复杂度为 O(log n)，能够在数百万节点规模下保持较高效率。
### 2.4 BitSwap 协议：去中心化内容交换机制
**协议概述**

在 IPFS 中，BitSwap 协议负责实现节点之间的数据交换与传输。
前面的内容寻址、Merkle DAG 和 DHT 共同解决了“内容是什么、由什么组成、在哪里找到”的问题，而 BitSwap 则解决“如何获取这些数据块”的问题。

换句话说，BitSwap 是一个去中心化的内容交换市场，节点既可以是消费者（请求数据），也可以是提供者（发送数据），
所有节点之间通过内容块进行互惠式的数据传输。

**运行流程**

BitSwap 协议的运行流程可以分为以下几个阶段：
1. 请求阶段
节点首先广播一个想要的内容列表，其中包含自己希望获取的块的 CID。
```
Node A → Wantlist: [QmABC, QmDEF]
```

2. 发现阶段
通过 DHT，节点找到可能持有这些块的节点（PeerID 列表）。

3. 交换阶段
节点之间通过 BitSwap 通信协议建立连接。
当节点发现自己拥有对方需要的块时，可以选择发送该块，从而获得积分。

4. 账本机制
每对节点之间维护一份本地账本，记录过去的发送与接收历史：
- 若一方长期只下载不上传，则积分会下降
- 另一方可能拒绝继续提供服务，从而形成公平的互惠关系

账本结构示例：
```
{
  peer: PeerID_B,
  sent: 12 blocks,
  received: 9 blocks,
  debtRatio: 12 / (9 + 1) = 1.2
}
```

5. 传输完成与缓存
当节点成功接收到所需的块后，会将其缓存到本地存储中。这使得节点在未来可以充当新的提供者，向其他节点传播同样的内容，从而形成一个内容自动扩散的网络生态。

